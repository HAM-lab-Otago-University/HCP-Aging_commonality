{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTANT !\n",
    "\n",
    "# In the first order need to set the number of CPU \n",
    "# for calculation before launching (depends on computer's number of cores)\n",
    "n_jobs= 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import joblib\n",
    "import warnings\n",
    "from datetime import date, datetime\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "import scipy.stats as st\n",
    "\n",
    "from nilearn import image as nli\n",
    "from nilearn import plotting\n",
    "\n",
    "from mne.viz import plot_connectivity_circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_features(table_in, control, index): \n",
    "    #table_in should be a table of features, where rows - subjects, columns - features\n",
    "    \n",
    "    if len(table_in.values.shape) == 1: #for pd.Series # for target\n",
    "        \n",
    "        #shrink data to local train index\n",
    "        table_in = table_in.reindex(index = index)\n",
    "        control = control.reindex(index = index)\n",
    "        ind = table_in.index\n",
    "        \n",
    "        #loop\n",
    "        dct_table = {}\n",
    "        dct_lin_models ={}\n",
    "        dct_std_y_models ={}\n",
    "        \n",
    "        col='0'\n",
    "        \n",
    "        y = table_in #target, brain ROI\n",
    "        X = control  #features, like age, sex and/or movements\n",
    "\n",
    "        #Standartize target\n",
    "        std_model_y = StandardScaler()\n",
    "        std_model_y.fit(y.values.reshape(-1, 1))\n",
    "        y = std_model_y.transform(y.values.reshape(-1, 1))\n",
    "        \n",
    "        #reshaping data\n",
    "        if len(X.values.shape) == 1:\n",
    "            X = X.values.reshape(-1, 1)\n",
    "        else:\n",
    "            X = X.values\n",
    "        y = y.reshape(-1, 1).ravel()\n",
    "        \n",
    "        #Standartize X\n",
    "        std_model = StandardScaler()\n",
    "        std_model.fit(X)\n",
    "        X = std_model.transform(X)\n",
    "\n",
    "        #Fit to the training set\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        y_pred = model.predict(X)\n",
    "\n",
    "        y_res = y - y_pred\n",
    "\n",
    "        dct_table[col] = y_res\n",
    "        dct_lin_models[col] = model\n",
    "        dct_std_y_models[col] = std_model_y\n",
    "\n",
    "        df_table = pd.DataFrame(dct_table, index = ind)\n",
    "\n",
    "        \n",
    "    else:\n",
    "            \n",
    "        #shrink data to local train index\n",
    "        table_in = table_in.reindex(index = index)\n",
    "        control = control.reindex(index = index)\n",
    "        ind = table_in.index\n",
    "\n",
    "        #loop\n",
    "        dct_table = {}\n",
    "        dct_lin_models ={}\n",
    "        dct_std_y_models ={}\n",
    "        col_names = table_in.columns\n",
    "\n",
    "        for col in col_names:\n",
    "            y = table_in[col] #target, brain ROI\n",
    "            X = control  #features, like age, sex and/or movements\n",
    "            \n",
    "            #Standartize target\n",
    "            std_model_y = StandardScaler()\n",
    "            std_model_y.fit(y.values.reshape(-1, 1))\n",
    "            y = std_model_y.transform(y.values.reshape(-1, 1)) \n",
    "            \n",
    "            #reshaping data\n",
    "            if len(X.values.shape) == 1:\n",
    "                X = X.values.reshape(-1, 1)\n",
    "            else:\n",
    "                X = X.values\n",
    "            y = y.reshape(-1, 1).ravel()\n",
    "            \n",
    "            #Standartize X\n",
    "            std_model = StandardScaler()\n",
    "            std_model.fit(X)\n",
    "            X = std_model.transform(X)\n",
    "\n",
    "            #Fit to the training set\n",
    "            model = LinearRegression()\n",
    "            model.fit(X, y)\n",
    "            y_pred = model.predict(X)\n",
    "\n",
    "            y_res = y - y_pred\n",
    "\n",
    "            dct_table[col] = y_res\n",
    "            dct_lin_models[col] = model\n",
    "            dct_std_y_models[col] = std_model_y\n",
    "\n",
    "        df_table = pd.DataFrame(dct_table, index = ind)\n",
    "    \n",
    "    return df_table, dct_std_y_models, std_model, dct_lin_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_control_features(table_in, control, index, dct_std_y_models, std_model, dct_lin_models):\n",
    "    \n",
    "    if len(table_in.values.shape) == 1: #for pd.Series # for target\n",
    "        \n",
    "        #shrink data to local train index\n",
    "        table_in = table_in.reindex(index = index)\n",
    "        control = control.reindex(index = index)\n",
    "        ind = table_in.index\n",
    "        \n",
    "        #loop\n",
    "        dct_table = {}\n",
    "        \n",
    "        col='0'\n",
    "        \n",
    "        y = table_in #target, brain ROI\n",
    "        X = control  #features, like age, sex and/or movements\n",
    "        \n",
    "        #standartize y\n",
    "        y = dct_std_y_models[col].transform(y.values.reshape(-1, 1))\n",
    "        \n",
    "        #reshaping data\n",
    "        if len(X.values.shape) == 1:\n",
    "            X = X.values.reshape(-1, 1)\n",
    "        else:\n",
    "            X = X.values\n",
    "        y = y.reshape(-1, 1).ravel()\n",
    "\n",
    "        #Standartize X with previous std model\n",
    "        X = std_model.transform(X)\n",
    "\n",
    "        #Fit with previous LinReg model\n",
    "        y_pred =  dct_lin_models[col].predict(X)\n",
    "\n",
    "        y_res = y - y_pred\n",
    "\n",
    "        dct_table[col] = y_res\n",
    "\n",
    "        df_table = pd.DataFrame(dct_table, index = ind)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #shrink data to local train index\n",
    "        table_in = table_in.reindex(index = index)\n",
    "        control = control.reindex(index = index)\n",
    "        ind = table_in.index\n",
    "\n",
    "        #loop\n",
    "        dct_table = {}\n",
    "        col_names = table_in.columns\n",
    "\n",
    "        for col in col_names:\n",
    "            y = table_in[col] #target, brain ROI\n",
    "            X = control  #features, like age, sex and/or movements\n",
    "\n",
    "            #standartize y\n",
    "            y = dct_std_y_models[col].transform(y.values.reshape(-1, 1))\n",
    "            \n",
    "            #reshaping data\n",
    "            if len(X.values.shape) == 1:\n",
    "                X = X.values.reshape(-1, 1)\n",
    "            else:\n",
    "                X = X.values\n",
    "            y = y.reshape(-1, 1).ravel()\n",
    "\n",
    "            #Standartize X with previous std model\n",
    "            X = std_model.transform(X)\n",
    "\n",
    "            #Fit with previous LinReg model\n",
    "            y_pred =  dct_lin_models[col].predict(X)\n",
    "\n",
    "            y_res = y - y_pred\n",
    "\n",
    "            dct_table[col] = y_res\n",
    "\n",
    "        df_table = pd.DataFrame(dct_table, index = ind)\n",
    "        \n",
    "    return df_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elnet(X, y):\n",
    "\n",
    "    #drop Nan in target and clean this subj from features\n",
    "    y = y.dropna()\n",
    "    X = X.loc[y.index,:]\n",
    "    ind_y = np.array(y.index)\n",
    "      \n",
    "    y_real=y\n",
    "    \n",
    "    #reshaping data\n",
    "    X = X.values\n",
    "    y = y.values.reshape(-1, 1).ravel()\n",
    "    \n",
    "    # Setup the pipeline steps:\n",
    "    steps = [('elasticnet', ElasticNet(random_state=42))]\n",
    "\n",
    "    # Create the pipeline: pipeline \n",
    "    pipeline = Pipeline(steps)\n",
    "\n",
    "    # Specify the hyperparameter space\n",
    "    parameters = {'elasticnet__alpha': np.logspace(-1, 2, 70),\n",
    "                  'elasticnet__l1_ratio':np.linspace(0,1,25)}\n",
    "\n",
    "    # Create the GridSearchCV object:\n",
    "    gm_cv = GridSearchCV(pipeline, parameters, cv=5, n_jobs=n_jobs)\n",
    "    \n",
    "    # Fit to the training set\n",
    "    gm_cv.fit(X, y)\n",
    "    \n",
    "    #predict new y\n",
    "    y_pred = gm_cv.predict(X)\n",
    "\n",
    "    # Compute and print the metrics\n",
    "    acc = gm_cv.best_score_\n",
    "    bpar = gm_cv.best_params_\n",
    "    model = gm_cv.best_estimator_\n",
    "    mse = mean_squared_error(y_real, y_pred)\n",
    "    mae = mean_absolute_error(y_real, y_pred)\n",
    "    corr, _ = pearsonr(np.array(y_real.values.reshape(-1, 1).ravel(), dtype=float), np.array(y_pred, dtype=float))\n",
    "            \n",
    "    return bpar['elasticnet__alpha'], bpar['elasticnet__l1_ratio'], acc, mse, corr, model, y_pred, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reaply_ElNet(X, y, model):\n",
    "    # param should be pd.Series with indexes from model\n",
    "    \n",
    "    #drop Nan in target and clean this subj from features\n",
    "    y = y.dropna()\n",
    "    X = X.reindex(index =y.index)\n",
    "    ind_y = np.array(y.index)  # indexes as separate variable \n",
    "    \n",
    "    y_real = y\n",
    "\n",
    "    #reshaping data\n",
    "    X = X.values\n",
    "    y = y.values.reshape(-1, 1).ravel()\n",
    "    \n",
    "    #predict new y\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Compute and print the metrics\n",
    "    bacc = model.score(X, y)\n",
    "    mse = mean_squared_error(y_real, y_pred)\n",
    "    mae = mean_absolute_error(y_real, y_pred) \n",
    "    corr, _ = pearsonr(np.array(y_real.values.reshape(-1, 1).ravel(), dtype=float), np.array(y_pred, dtype=float))\n",
    "    \n",
    "    return y_pred, y_real, ind_y, bacc, mse, corr, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to the tables folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/media/data/HCPAging/data/MLTablesMultCon/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "#demography\n",
    "demo = pd.read_csv(path+'demography.csv', index_col=0)\n",
    "\n",
    "#targets table\n",
    "targ = pd.read_csv(path+'cognition.csv', index_col=0)\n",
    "\n",
    "#features tables as dictionary\n",
    "features = {\n",
    "    'carit1':pd.read_csv(path+'carit-con1.csv', index_col=0),\n",
    "    'carit3':pd.read_csv(path+'carit-con3.csv', index_col=0),\n",
    "    'carit4':pd.read_csv(path+'carit-con4.csv', index_col=0),\n",
    "    \n",
    "    'face1':pd.read_csv(path+'FACENAME_group_table_3EV_con1.csv', index_col=0),\n",
    "    'face2':pd.read_csv(path+'FACENAME_group_table_3EV_con2.csv', index_col=0),\n",
    "    'face3':pd.read_csv(path+'FACENAME_group_table_3EV_con3.csv', index_col=0),\n",
    "    'face4':pd.read_csv(path+'FACENAME_group_table_3EV_con4.csv', index_col=0),\n",
    "    'face5':pd.read_csv(path+'FACENAME_group_table_3EV_con5.csv', index_col=0),\n",
    "    'face6':pd.read_csv(path+'FACENAME_group_table_3EV_con6.csv', index_col=0),\n",
    "    \n",
    "    'vism':pd.read_csv(path+'vism.csv', index_col=0),\n",
    "    \n",
    "    'carit_FC':pd.read_csv(path+'CARIT_taskFC.csv', index_col=0),\n",
    "    'face_FC':pd.read_csv(path+'FACENAME_task_FC_3EV.csv', index_col=0),\n",
    "    'vism_FC':pd.read_csv(path+'VISMOTOR_taskFC.csv', index_col=0),\n",
    "\n",
    "    'cort':pd.read_csv(path+'cort.csv', index_col=0),\n",
    "    'surf':pd.read_csv(path+'surf.csv', index_col=0),\n",
    "    'subc':pd.read_csv(path+'subc.csv', index_col=0),\n",
    "    'VolBrain':pd.read_csv(path+'VolBrain.csv', index_col=0),\n",
    "    \n",
    "    'rest':pd.read_csv(path+'rest_hpass.csv', index_col=0) \n",
    "\n",
    "}\n",
    "\n",
    "#table with movements (mean relative displacement Movement_RelativeRMS_mean.txt)\n",
    "movements = pd.read_csv(path+'movements.csv', index_col=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tables withcontroling parameters\n",
    "sex_coded = pd.Series(LabelEncoder().fit_transform(demo.loc[:,['sex']]), index=demo.index, name='sex')\n",
    "\n",
    "control = pd.DataFrame({'sex':sex_coded}) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Leave-P-group out based on n-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interview_age\n",
      " \n",
      "started to calculate the Fold # 0\n",
      "2023-07-30 17:12:52.151844\n",
      " \n",
      "start 1st level  2023-07-30 17:12:52.193515\n",
      "controlling  carit1 2023-07-30 17:12:52.194209\n",
      "controlling  carit3 2023-07-30 17:12:52.525843\n",
      "controlling  carit4 2023-07-30 17:12:52.803113\n",
      "controlling  face1 2023-07-30 17:12:53.081598\n",
      "controlling  face2 2023-07-30 17:12:53.360585\n",
      "controlling  face3 2023-07-30 17:12:53.640981\n",
      "controlling  face4 2023-07-30 17:12:53.920265\n",
      "controlling  face5 2023-07-30 17:12:54.200144\n",
      "controlling  face6 2023-07-30 17:12:54.472638\n",
      "controlling  vism 2023-07-30 17:12:54.709204\n",
      "controlling  carit_FC 2023-07-30 17:12:54.946743\n",
      "controlling  face_FC 2023-07-30 17:13:42.204006\n",
      "controlling  vism_FC 2023-07-30 17:14:30.311561\n",
      "controlling  cort 2023-07-30 17:15:18.492639\n",
      "controlling  surf 2023-07-30 17:15:18.657118\n",
      "controlling  subc 2023-07-30 17:15:18.803139\n",
      "controlling  VolBrain 2023-07-30 17:15:18.819185\n",
      "controlling  rest 2023-07-30 17:15:18.824122\n",
      "standartize  carit1 2023-07-30 17:17:47.483622\n",
      "standartize  carit3 2023-07-30 17:17:47.486821\n",
      "standartize  carit4 2023-07-30 17:17:47.489372\n",
      "standartize  face1 2023-07-30 17:17:47.490884\n",
      "standartize  face2 2023-07-30 17:17:47.492183\n",
      "standartize  face3 2023-07-30 17:17:47.493489\n",
      "standartize  face4 2023-07-30 17:17:47.494993\n",
      "standartize  face5 2023-07-30 17:17:47.496481\n",
      "standartize  face6 2023-07-30 17:17:47.497963\n",
      "standartize  vism 2023-07-30 17:17:47.499706\n",
      "standartize  carit_FC 2023-07-30 17:17:47.501788\n",
      "standartize  face_FC 2023-07-30 17:17:47.779830\n",
      "standartize  vism_FC 2023-07-30 17:17:48.053974\n",
      "standartize  cort 2023-07-30 17:17:48.328709\n",
      "standartize  surf 2023-07-30 17:17:48.330290\n",
      "standartize  subc 2023-07-30 17:17:48.331279\n",
      "standartize  VolBrain 2023-07-30 17:17:48.332040\n",
      "standartize  rest 2023-07-30 17:17:48.332724\n",
      "reduction  rest 2023-07-30 17:23:32.050826\n",
      "reduction  carit_FC 2023-07-30 17:23:33.029537\n",
      "reduction  face_FC 2023-07-30 17:23:33.988367\n",
      "reduction  vism_FC 2023-07-30 17:23:34.932115\n",
      "standartize PC table  rest 2023-07-30 17:23:36.137115\n",
      "standartize PC table  carit_FC 2023-07-30 17:23:36.182375\n",
      "standartize PC table  face_FC 2023-07-30 17:23:36.219167\n",
      "standartize PC table  vism_FC 2023-07-30 17:23:36.257079\n",
      "start  carit1 2023-07-30 17:23:36.373457\n",
      "start  carit3 2023-07-30 17:23:50.840630\n",
      "start  carit4 2023-07-30 17:24:05.667520\n",
      "start  face1 2023-07-30 17:24:19.503304\n",
      "start  face2 2023-07-30 17:24:33.336018\n",
      "start  face3 2023-07-30 17:24:47.309057\n",
      "start  face4 2023-07-30 17:25:01.175595\n",
      "start  face5 2023-07-30 17:25:15.619873\n",
      "start  face6 2023-07-30 17:25:29.390632\n",
      "start  vism 2023-07-30 17:25:43.121712\n",
      "start  carit_FC 2023-07-30 17:25:56.460779\n",
      "start  face_FC 2023-07-30 17:26:02.534148\n",
      "start  vism_FC 2023-07-30 17:26:10.052018\n",
      "start  cort 2023-07-30 17:26:17.263287\n",
      "start  surf 2023-07-30 17:26:25.467414\n",
      "start  subc 2023-07-30 17:26:34.272695\n",
      "start  VolBrain 2023-07-30 17:26:39.561762\n",
      "start  rest 2023-07-30 17:26:45.429294\n",
      " \n",
      "start 2nd level  2023-07-30 17:26:51.667036\n",
      "Checking single ML on train 40prc data  2023-07-30 17:26:51.667106\n",
      "controlling  carit1 2023-07-30 17:26:51.668417\n",
      "controlling  carit3 2023-07-30 17:26:51.739264\n",
      "controlling  carit4 2023-07-30 17:26:51.803173\n",
      "controlling  face1 2023-07-30 17:26:51.867007\n",
      "controlling  face2 2023-07-30 17:26:51.931346\n",
      "controlling  face3 2023-07-30 17:26:51.995147\n",
      "controlling  face4 2023-07-30 17:26:52.059522\n",
      "controlling  face5 2023-07-30 17:26:52.127197\n",
      "controlling  face6 2023-07-30 17:26:52.192742\n",
      "controlling  vism 2023-07-30 17:26:52.256001\n",
      "controlling  carit_FC 2023-07-30 17:26:52.319354\n",
      "controlling  face_FC 2023-07-30 17:27:06.593906\n",
      "controlling  vism_FC 2023-07-30 17:27:19.846725\n",
      "controlling  cort 2023-07-30 17:27:33.354116\n",
      "controlling  surf 2023-07-30 17:27:33.400076\n",
      "controlling  subc 2023-07-30 17:27:33.424763\n",
      "controlling  VolBrain 2023-07-30 17:27:33.428658\n",
      "controlling  rest 2023-07-30 17:27:33.430336\n",
      "standartize  carit1 2023-07-30 17:27:47.005928\n",
      "standartize  carit3 2023-07-30 17:27:47.012355\n",
      "standartize  carit4 2023-07-30 17:27:47.018679\n",
      "standartize  face1 2023-07-30 17:27:47.025911\n",
      "standartize  face2 2023-07-30 17:27:47.030998\n",
      "standartize  face3 2023-07-30 17:27:47.031913\n",
      "standartize  face4 2023-07-30 17:27:47.032884\n",
      "standartize  face5 2023-07-30 17:27:47.033847\n",
      "standartize  face6 2023-07-30 17:27:47.034832\n",
      "standartize  vism 2023-07-30 17:27:47.035855\n",
      "standartize  carit_FC 2023-07-30 17:27:47.036820\n",
      "standartize  face_FC 2023-07-30 17:27:47.145893\n",
      "standartize  vism_FC 2023-07-30 17:27:47.254630\n",
      "standartize  cort 2023-07-30 17:27:47.363943\n",
      "standartize  surf 2023-07-30 17:27:47.364885\n",
      "standartize  subc 2023-07-30 17:27:47.365537\n",
      "standartize  VolBrain 2023-07-30 17:27:47.366052\n",
      "standartize  rest 2023-07-30 17:27:47.366514\n",
      "reduction  rest 2023-07-30 17:33:49.367437\n",
      "reduction  carit_FC 2023-07-30 17:33:49.497160\n",
      "reduction  face_FC 2023-07-30 17:33:49.631375\n",
      "reduction  vism_FC 2023-07-30 17:33:49.738577\n",
      "standartize PCA  rest 2023-07-30 17:33:49.845425\n",
      "standartize PCA  carit_FC 2023-07-30 17:33:49.889458\n",
      "standartize PCA  face_FC 2023-07-30 17:33:49.924097\n",
      "standartize PCA  vism_FC 2023-07-30 17:33:49.958478\n",
      "Calculating stacked ML on train 40prc data  2023-07-30 17:33:50.020096\n",
      "set 1 2023-07-30 17:33:50.020740\n",
      "set 2 2023-07-30 17:33:56.791847\n",
      "set 3 2023-07-30 17:34:03.459274\n",
      "set 4 2023-07-30 17:34:10.054903\n",
      "set 5 2023-07-30 17:34:17.656311\n",
      "set 6 2023-07-30 17:34:24.097335\n",
      "set 7 2023-07-30 17:34:29.790395\n",
      "set 8 2023-07-30 17:34:35.887519\n",
      " \n",
      "start 3rd level  2023-07-30 17:34:43.574340\n",
      "Checking single ML on test data  2023-07-30 17:34:43.574439\n",
      "controlling  carit1 2023-07-30 17:34:43.575090\n",
      "controlling  carit3 2023-07-30 17:34:43.644150\n",
      "controlling  carit4 2023-07-30 17:34:43.707305\n",
      "controlling  face1 2023-07-30 17:34:43.770549\n",
      "controlling  face2 2023-07-30 17:34:43.834578\n",
      "controlling  face3 2023-07-30 17:34:43.897810\n",
      "controlling  face4 2023-07-30 17:34:43.961020\n",
      "controlling  face5 2023-07-30 17:34:44.026404\n",
      "controlling  face6 2023-07-30 17:34:44.095495\n",
      "controlling  vism 2023-07-30 17:34:44.163181\n",
      "controlling  carit_FC 2023-07-30 17:34:44.226711\n",
      "controlling  face_FC 2023-07-30 17:34:57.871597\n",
      "controlling  vism_FC 2023-07-30 17:35:11.417252\n",
      "controlling  cort 2023-07-30 17:35:24.779540\n",
      "controlling  surf 2023-07-30 17:35:25.112629\n",
      "controlling  subc 2023-07-30 17:35:25.137981\n",
      "controlling  VolBrain 2023-07-30 17:35:25.141840\n",
      "controlling  rest 2023-07-30 17:35:25.143404\n",
      "standartize  carit1 2023-07-30 17:35:38.690651\n",
      "standartize  carit3 2023-07-30 17:35:38.697026\n",
      "standartize  carit4 2023-07-30 17:35:38.703533\n",
      "standartize  face1 2023-07-30 17:35:38.709681\n",
      "standartize  face2 2023-07-30 17:35:38.714272\n",
      "standartize  face3 2023-07-30 17:35:38.715238\n",
      "standartize  face4 2023-07-30 17:35:38.716192\n",
      "standartize  face5 2023-07-30 17:35:38.717153\n",
      "standartize  face6 2023-07-30 17:35:38.718126\n",
      "standartize  vism 2023-07-30 17:35:38.719252\n",
      "standartize  carit_FC 2023-07-30 17:35:38.720417\n",
      "standartize  face_FC 2023-07-30 17:35:38.828906\n",
      "standartize  vism_FC 2023-07-30 17:35:38.937328\n",
      "standartize  cort 2023-07-30 17:35:39.046316\n",
      "standartize  surf 2023-07-30 17:35:39.047288\n",
      "standartize  subc 2023-07-30 17:35:39.047979\n",
      "standartize  VolBrain 2023-07-30 17:35:39.048572\n",
      "standartize  rest 2023-07-30 17:35:39.049153\n",
      "reduction  rest 2023-07-30 17:41:41.953705\n",
      "reduction  carit_FC 2023-07-30 17:41:42.093840\n",
      "reduction  face_FC 2023-07-30 17:41:42.228086\n",
      "reduction  vism_FC 2023-07-30 17:41:42.359675\n",
      "standartize PCA  rest 2023-07-30 17:41:42.499113\n",
      "standartize PCA  carit_FC 2023-07-30 17:41:42.545697\n",
      "standartize PCA  face_FC 2023-07-30 17:41:42.588520\n",
      "standartize PCA  vism_FC 2023-07-30 17:41:42.630681\n",
      "Calculating stacked ML on test2 data  2023-07-30 17:41:42.700660\n",
      " \n",
      "finished to calculate the Fold # 0\n",
      "2023-07-30 17:41:42.906347\n",
      " \n",
      "finished the MODEL interview_age\n",
      "2023-07-30 17:41:42.906452\n"
     ]
    }
   ],
   "source": [
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#for COL in targ.columns:\n",
    "COL = 'interview_age'#'nih_fluidcogcomp_unadjusted'  #the script adapted to be launched on table of target variables. To launch in that way you need to uncomment for loop and comment this row with col variable\n",
    "y = demo[COL]#targ#targ[COL]\n",
    "\n",
    "print(y.name)\n",
    "\n",
    "###make folder for outputs\n",
    "nmf=path+'output_5cv_sexAdj_noStdTarg_STDstackFeatures_SHAP_'+y.name\n",
    "os.mkdir(nmf)\n",
    "\n",
    "i=0\n",
    "\n",
    "#group_kfold = GroupKFold(n_splits=5) #number of folds\n",
    "#for train_index, test_index in group_kfold.split(demo, groups=demo['family_user_def_id']): #based on families id\n",
    "\n",
    "print(' ')\n",
    "print('started to calculate the Fold #', i)\n",
    "print(datetime.now())\n",
    "print(' ')\n",
    "\n",
    "###create directory for specific Fold\n",
    "os.mkdir(nmf+'/Fold_'+str(i)) \n",
    "path_out = str(nmf+'/Fold_'+str(i))\n",
    "\n",
    "###Global indices\n",
    "#train_index = np.array(demo.iloc[train_index].index) #for training all models\n",
    "#test_index = np.array(demo.iloc[test_index].index) #for final test\n",
    "train_index = np.array(demo.index) #for training all models\n",
    "test_index = np.array(demo.index) #for final test    \n",
    "\n",
    "###Split global to local indices (in case if we need to split training part into two as previously)\n",
    "#index_train, index_test = train_test_split(train_index, test_size=0.4, random_state=42)\n",
    "#index_train = np.array(sorted(index_train)) #for training modalities models\n",
    "#index_test = np.array(sorted(index_test)) #for testing modalities and training stacking\n",
    "\n",
    "### 1st level ################################################################################\n",
    "\n",
    "#### Calculations of single ML models on training index #################################### \n",
    "\n",
    "print('start 1st level ', datetime.now())\n",
    "\n",
    "\n",
    "#reindex y (target)\n",
    "y_res1 = y.reindex(index=train_index)\n",
    "\n",
    "\n",
    "#control modalities\n",
    "features_res1 = {}\n",
    "std_feat_y_dct = {}\n",
    "std_feat_X_dct = {}\n",
    "linreg_feat_dct = {}\n",
    "for key in features.keys():\n",
    "    print('controlling ', key, datetime.now())\n",
    "\n",
    "    mod_res, std_f_y, std_f_X, linreg_f = control_features(features[key], control, y_res1.index)\n",
    "\n",
    "    features_res1[key] = mod_res\n",
    "    std_feat_y_dct[key] = std_f_y\n",
    "    std_feat_X_dct[key] = std_f_X\n",
    "    linreg_feat_dct[key] = linreg_f\n",
    "\n",
    "#save adjastment model\n",
    "os.mkdir(path_out+'/adjustment_models')\n",
    "#features model\n",
    "joblib.dump(std_feat_y_dct, (path_out+'/adjustment_models'+'/features_std_model_y.sav'))\n",
    "joblib.dump(std_feat_X_dct, (path_out+'/adjustment_models'+'/features_std_model_X.sav'))\n",
    "joblib.dump(linreg_feat_dct, (path_out+'/adjustment_models'+'/features_linreg.sav'))\n",
    "\n",
    "\n",
    "###standartize before model and keep std models\n",
    "#features\n",
    "std_models_features = {}\n",
    "for key in features_res1.keys():\n",
    "    print('standartize ', key, datetime.now())\n",
    "    std_model = StandardScaler()\n",
    "    std_model.fit(features_res1[key].values)\n",
    "    features_res1[key] = pd.DataFrame(std_model.transform(features_res1[key].values),\n",
    "                                      index=features_res1[key].index, \n",
    "                                      columns=features_res1[key].columns)\n",
    "    std_models_features[key] = std_model\n",
    "\n",
    "\n",
    "#save \n",
    "os.mkdir(path_out+'/standartization_models')\n",
    "#features\n",
    "joblib.dump(std_models_features,  (path_out+'/standartization_models'+'/features_std_model.sav'))\n",
    "\n",
    "\n",
    "#save features table before PCA\n",
    "y_res1.to_csv(path_out+'/target_y_train1.csv')\n",
    "for key in features_res1.keys():\n",
    "    features_res1[key].to_csv(path_out+'/'+str(key)+'_train1.csv')\n",
    "\n",
    "\n",
    "#PCA models to rest and task FC\n",
    "PCA_models = {}\n",
    "for key in ['rest', 'carit_FC', 'face_FC', 'vism_FC']:\n",
    "    print('reduction ', key, datetime.now())\n",
    "    model_PCA =  PCA(n_components=75, random_state=11)\n",
    "    model_PCA.fit(features_res1[key].values)\n",
    "    features_res1[key] = pd.DataFrame(model_PCA.transform(features_res1[key].values), \n",
    "                                      index=features_res1[key].index)\n",
    "    PCA_models[key] = model_PCA\n",
    "#save PCA models\n",
    "os.mkdir(path_out+'/PCA_models')\n",
    "joblib.dump(PCA_models,  (path_out+'/PCA_models'+'/PCA_model.sav'))\n",
    "\n",
    "\n",
    "#apply new std to PCA features again\n",
    "std_PC_feature_models = {}\n",
    "for key in ['rest', 'carit_FC', 'face_FC', 'vism_FC']:\n",
    "    print('standartize PC table ', key, datetime.now())\n",
    "    std_PC_model = StandardScaler()\n",
    "    std_PC_model.fit(features_res1[key].values)\n",
    "    features_res1[key] = pd.DataFrame(std_PC_model.transform(features_res1[key].values),\n",
    "                                      index=features_res1[key].index, \n",
    "                                      columns=features_res1[key].columns)\n",
    "    std_PC_feature_models[key] = std_PC_model\n",
    "    #save PCA tables\n",
    "    features_res1[key].to_csv(path_out+'/'+key+'_PCA75_train1.csv')\n",
    "\n",
    "#save std PCA models\n",
    "os.mkdir(path_out+'/PCA_standardization_models')\n",
    "joblib.dump(std_PC_feature_models,  (path_out+'/PCA_standardization_models'+'/std_PCA_model.sav'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Launch ElasticNet for all task(modalities) on index_train (1st level)\n",
    "\n",
    "dict_tasks={}\n",
    "dict_elnet_model={}\n",
    "dict_ypred1={}\n",
    "\n",
    "for key in list(features_res1.keys()):\n",
    "\n",
    "    print('start ', str(key), datetime.now())   #print start time of calculations\n",
    "\n",
    "    bpar1, bpar2, acc, mse, corr, model, y_pred1, mae = elnet(features_res1[key], y_res1) #ML\n",
    "    dict_tasks[key] = acc, mse, mae, corr, bpar1, bpar2 \n",
    "    dict_elnet_model[key] = model\n",
    "    dict_ypred1[key] = y_pred1\n",
    "df_tasks = pd.DataFrame(dict_tasks, index=['best score r2', 'mse', 'mae','corr', 'best alpha', 'best l1_ratio'])\n",
    "df_y_pred1 = pd.DataFrame(dict_ypred1, index=y_res1.index)\n",
    "\n",
    "\n",
    "###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "#models\n",
    "for key in dict_elnet_model.keys():\n",
    "    joblib.dump(dict_elnet_model[key], (path_out+'/'+str(key)+'_elnet_model.sav'))\n",
    "\n",
    "#model performance\n",
    "df_tasks.to_csv(path_out+'/1level_train_perf_elnet.csv')\n",
    "\n",
    "#list of first level targets (observed and predicted)\n",
    "df_y_pred1.to_csv(path_out+'/1level_train_y_pred_singleML.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2st level ################################################################################\n",
    "print(' ')\n",
    "print('start 2nd level ', datetime.now())\n",
    "\n",
    "#### L2 Testing single ML models on 40pr of training (in no 60/40 splitting it just repeats previous)###################\n",
    "\n",
    "print('Checking single ML on train 40prc data ', datetime.now())\n",
    "\n",
    "#controlling  with sorting to index_test\n",
    "\n",
    "#reindex y (target) \n",
    "y_res2 = y.reindex(index=train_index) \n",
    "\n",
    "#control modalities\n",
    "features_res2 = {}\n",
    "for key in features.keys():\n",
    "    print('controlling ', key, datetime.now())\n",
    "\n",
    "    features_res2[key] = re_control_features(features[key], control, y_res2.index, \n",
    "                                             std_feat_y_dct[key], std_feat_X_dct[key], linreg_feat_dct[key])\n",
    "\n",
    "###standartize before model and keep std models\n",
    "#features\n",
    "for key in features_res2.keys():\n",
    "    print('standartize ', key, datetime.now())\n",
    "    features_res2[key] = pd.DataFrame(std_models_features[key].transform(features_res2[key].values),\n",
    "                                      index=features_res2[key].index, \n",
    "                                      columns=features_res2[key].columns) \n",
    "\n",
    "#save features table before PCA\n",
    "y_res2.to_csv(path_out+'/target_y_train2.csv')\n",
    "for key in features_res2.keys():\n",
    "    features_res2[key].to_csv(path_out+'/'+str(key)+'_train2.csv')            \n",
    "\n",
    "\n",
    "#PCA models to rest and task FC\n",
    "for key in ['rest', 'carit_FC', 'face_FC', 'vism_FC']:\n",
    "    print('reduction ', key, datetime.now())\n",
    "    features_res2[key] = pd.DataFrame(PCA_models[key].transform(features_res2[key].values),\n",
    "                                      index=features_res2[key].index)\n",
    "\n",
    "\n",
    "#apply new std to PCA features again\n",
    "for key in ['rest', 'carit_FC', 'face_FC', 'vism_FC']:\n",
    "    print('standartize PCA ', key, datetime.now())\n",
    "    features_res2[key] = pd.DataFrame(std_PC_feature_models[key].transform(features_res2[key].values),\n",
    "                                      index=features_res2[key].index, \n",
    "                                      columns=features_res2[key].columns)\n",
    "    #save std pc table\n",
    "    features_res2[key].to_csv(path_out+'/'+key+'_PCA75_train2.csv')\n",
    "\n",
    "\n",
    "#apply trained single models ElasticNet to new subset\n",
    "\n",
    "dict_y_pred2={}\n",
    "dict_y_pred2_per={}\n",
    "for key in list(features_res2.keys()):\n",
    "    y_pred, y_real, ind_y, bacc, mse, corr, mae = reaply_ElNet(features_res2[key], y_res2, \n",
    "                                                               dict_elnet_model[key]) #ML\n",
    "    dict_y_pred2[key] = y_pred\n",
    "    dict_y_pred2_per[key] = bacc, mse, mae, corr\n",
    "\n",
    "df_y_pred2 = pd.DataFrame(dict_y_pred2, index=ind_y)\n",
    "df_y_pred2_per = pd.DataFrame(dict_y_pred2_per, index=['best score r2', 'mse', 'mae','corr'])\n",
    "\n",
    "\n",
    "###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "#model performance\n",
    "df_y_pred2_per.to_csv(path_out+'/2level_test1_perf_elnet.csv')\n",
    "\n",
    "#list of first level targets (observed and predicted)\n",
    "df_y_pred2.to_csv(path_out+'/2level_test1_y_pred_singleML.csv')   \n",
    "\n",
    "\n",
    "\n",
    "#### L2 Calculating stacked ML models on 40prc #############################################\n",
    "\n",
    "print('Calculating stacked ML on train 40prc data ', datetime.now())    \n",
    "\n",
    "\n",
    "#identifying sets for several stacked models\n",
    "set2 = ['carit1', 'carit3', 'carit4', 'face1', 'face2', 'face3', 'face4', 'face5', 'face6', 'vism']\n",
    "set3 = ['cort', 'subc', 'surf', 'rest', 'VolBrain']\n",
    "\n",
    "set4 = ['carit_FC', 'face_FC', 'vism_FC']\n",
    "set5 = ['carit1', 'carit3', 'carit4', 'face1', 'face2', 'face3', 'face4', 'face5', 'face6', 'vism', 'carit_FC', 'face_FC', 'vism_FC']\n",
    "set6 = ['carit1', 'carit3', 'carit4', 'face1', 'face2', 'face3', 'face4', 'face5', 'face6', 'vism', 'cort', 'subc', 'surf', 'rest', 'VolBrain']\n",
    "set7 = ['carit_FC', 'face_FC', 'vism_FC', 'cort', 'subc', 'surf', 'rest', 'VolBrain']\n",
    "set8 = ['carit_FC', 'face_FC', 'vism_FC', 'rest']\n",
    "\n",
    "set1 = list(df_y_pred2.columns) #all existed modalities\n",
    "\n",
    "#for presetet sets\n",
    "dict_st_perf1={}\n",
    "dict_st_models={}\n",
    "dict_st_ypred1={}\n",
    "dct_std_mod_for_stack = {} #\n",
    "dct_std_tab_for_stack = {} #\n",
    "dct_std_tab_before_for_stack = {} #\n",
    "\n",
    "s=1\n",
    "for set_n in [set1, set2, set3, set4, set5, set6, set7, set8]:\n",
    "    print('set '+str(s), datetime.now())\n",
    "\n",
    "    #standardize stacking featue table and save std model for next subset\n",
    "    st_features = df_y_pred2.loc[:,set_n]\n",
    "    dct_std_tab_before_for_stack['set'+str(s)] = st_features #\n",
    "\n",
    "    stack_std_model = StandardScaler().fit(st_features.values) \n",
    "    dct_std_mod_for_stack['set'+str(s)] = stack_std_model #\n",
    "\n",
    "    std_st_features = pd.DataFrame(stack_std_model.transform(st_features.values), \n",
    "                                   index=st_features.index, columns=st_features.columns) \n",
    "    dct_std_tab_for_stack['set'+str(s)] = std_st_features #\n",
    "\n",
    "\n",
    "\n",
    "    bpar1, bpar2, acc, mse, corr, model, y_pred3, mae = elnet(std_st_features, y_res2) #ML\n",
    "\n",
    "    dict_st_perf1['set'+str(s)] = acc, mse, mae, corr, bpar1, bpar2 \n",
    "    dict_st_models['set'+str(s)] = model\n",
    "    dict_st_ypred1['set'+str(s)] = y_pred3\n",
    "    s+=1\n",
    "\n",
    "df_st_perf1 = pd.DataFrame(dict_st_perf1, index=['best score r2', 'mse', 'mae','corr', 'best alpha', 'best l1_ratio'])\n",
    "df_st_ypred1 = pd.DataFrame(dict_st_ypred1, index=y_res2.index)        \n",
    "\n",
    "###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "#models\n",
    "for key in dict_st_models.keys():\n",
    "    joblib.dump(dict_st_models[key], (path_out+'/'+str(key)+'_stacked_model.sav'))\n",
    "for key in dct_std_mod_for_stack.keys():\n",
    "    joblib.dump(dct_std_mod_for_stack[key], (path_out+'/'+str(key)+'_stacked_STD_model.sav'))\n",
    "\n",
    "#performance and prediction\n",
    "df_st_perf1.to_csv(path_out+'/2level_test1_perf_stacked.csv')\n",
    "df_st_ypred1.to_csv(path_out+'/2level_test1_y_pred_stacked.csv')\n",
    "for key in dct_std_tab_for_stack.keys():\n",
    "    dct_std_tab_for_stack[key].to_csv(path_out+'/2level_stack_y_feature_tab_STD.csv')\n",
    "    dct_std_tab_before_for_stack[key].to_csv(path_out+'/2level_stack_y_feature_tab_beforeSTD.csv')\n",
    "\n",
    "\n",
    "### 3rd level ################################################################################\n",
    "print(' ')\n",
    "print('start 3rd level ', datetime.now())\n",
    "\n",
    "\n",
    "#### L3 Testing single ML models on test_index #############################################\n",
    "\n",
    "print('Checking single ML on test data ', datetime.now())\n",
    "\n",
    "#controlling with sorting to test index\n",
    "\n",
    "#control y (target)\n",
    "y_res3 = y.reindex(index=test_index)\n",
    "\n",
    "#control modalities\n",
    "features_res3 = {}\n",
    "for key in features.keys():\n",
    "    print('controlling ', key, datetime.now())\n",
    "\n",
    "    features_res3[key] = re_control_features(features[key], control, y_res3.index, \n",
    "                                             std_feat_y_dct[key], std_feat_X_dct[key], linreg_feat_dct[key])\n",
    "\n",
    "###standartize before model and keep std models\n",
    "#features\n",
    "for key in features_res3.keys():\n",
    "    print('standartize ', key, datetime.now())\n",
    "    features_res3[key] = pd.DataFrame(std_models_features[key].transform(features_res3[key].values),\n",
    "                                      index=features_res3[key].index, \n",
    "                                      columns=features_res3[key].columns)\n",
    "\n",
    "\n",
    "#save features table before PCA\n",
    "y_res3.to_csv(path_out+'/target_y_test.csv')\n",
    "for key in features_res3.keys():\n",
    "    features_res3[key].to_csv(path_out+'/'+str(key)+'_test.csv')            \n",
    "\n",
    "\n",
    "#PCA models to rest and task FC\n",
    "for key in ['rest', 'carit_FC', 'face_FC', 'vism_FC']:\n",
    "    print('reduction ', key, datetime.now())\n",
    "    features_res3[key] = pd.DataFrame(PCA_models[key].transform(features_res3[key].values),\n",
    "                                      index=features_res3[key].index)\n",
    "\n",
    "\n",
    "#apply new std to PCA features again\n",
    "for key in ['rest', 'carit_FC', 'face_FC', 'vism_FC']:\n",
    "    print('standartize PCA ', key, datetime.now())\n",
    "    features_res3[key] = pd.DataFrame(std_PC_feature_models[key].transform(features_res3[key].values),\n",
    "                                      index=features_res3[key].index, \n",
    "                                      columns=features_res3[key].columns)\n",
    "    #save std pc table\n",
    "    features_res3[key].to_csv(path_out+'/'+key+'_PCA75_test.csv')\n",
    "\n",
    "\n",
    "\n",
    "#apply trained single models ElasticNet to new subset\n",
    "\n",
    "dict_y_pred3={}\n",
    "dict_y_pred3_per={}\n",
    "for key in list(features_res3.keys()):\n",
    "    y_pred, y_real, ind_y, bacc, mse, corr, mae = reaply_ElNet(features_res3[key], y_res3, dict_elnet_model[key]) #ML\n",
    "    dict_y_pred3[key] = y_pred\n",
    "    dict_y_pred3_per[key] = bacc, mse, mae, corr\n",
    "\n",
    "df_y_pred3 = pd.DataFrame(dict_y_pred3, index=ind_y)\n",
    "df_y_pred3_per = pd.DataFrame(dict_y_pred3_per, index=['best score r2', 'mse', 'mae','corr'])\n",
    "\n",
    "\n",
    "###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "#model performance\n",
    "df_y_pred3_per.to_csv(path_out+'/3level_test2_perf_elnet.csv')\n",
    "\n",
    "#list of first level targets (observed and predicted)\n",
    "df_y_pred3.to_csv(path_out+'/3level_test2_y_pred_singleML.csv')        \n",
    "\n",
    "\n",
    "#### L3 Testing stacked ML models on test_index #############################################\n",
    "\n",
    "print('Calculating stacked ML on test2 data ', datetime.now()) \n",
    "\n",
    "#apply trained stacked models ElasticNet to new data , test_index\n",
    "\n",
    "#for presetet sets\n",
    "dict_st_perf2={}\n",
    "dict_st_ypred2={}\n",
    "\n",
    "dct_std3_tab_for_stack = {} #\n",
    "dct_std3_tab_before_for_stack = {} #\n",
    "\n",
    "s=1\n",
    "for set_n in [set1, set2, set3, set4, set5, set6, set7, set8]:\n",
    "    #standardize stacking feature\n",
    "    ftrs = df_y_pred3.loc[:, set_n]\n",
    "    dct_std3_tab_before_for_stack['set'+str(s)] = ftrs\n",
    "\n",
    "    std_ftrs = pd.DataFrame(dct_std_mod_for_stack['set'+str(s)].transform(ftrs.values), \n",
    "                            index=ftrs.index,columns=ftrs.columns)\n",
    "    dct_std3_tab_for_stack['set'+str(s)] = std_ftrs\n",
    "    #ML\n",
    "    y_pred, y_real, ind_y, bacc, mse, corr, mae = reaply_ElNet(std_ftrs, y_res3, dict_st_models[('set'+str(s))]) #ML\n",
    "    dict_st_ypred2[('set'+str(s))] = y_pred\n",
    "    dict_st_perf2[('set'+str(s))] = bacc, mse, mae, corr\n",
    "    s+=1\n",
    "\n",
    "df_st_ypred2 = pd.DataFrame(dict_st_ypred2, index=ind_y)\n",
    "df_st_perf2 = pd.DataFrame(dict_st_perf2, index=['best score r2', 'mse', 'mae','corr'])        \n",
    "\n",
    "###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "#performance and prediction\n",
    "df_st_perf2.to_csv(path_out+'/3level_test2_perf_stacked.csv')\n",
    "df_st_ypred2.to_csv(path_out+'/3level_test2_y_pred_stacked.csv') \n",
    "for key in dct_std3_tab_for_stack.keys():\n",
    "    dct_std3_tab_for_stack[key].to_csv(path_out+'/3level_stack_y_feature_tab_STD.csv')\n",
    "    dct_std3_tab_before_for_stack[key].to_csv(path_out+'/3level_stack_y_feature_tab_beforeSTD.csv')\n",
    "\n",
    "print(' ')\n",
    "print('finished to calculate the Fold #', i)\n",
    "print(datetime.now())\n",
    "\n",
    "i+=1\n",
    "\n",
    "print(' ')\n",
    "print('finished the MODEL '+COL)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
