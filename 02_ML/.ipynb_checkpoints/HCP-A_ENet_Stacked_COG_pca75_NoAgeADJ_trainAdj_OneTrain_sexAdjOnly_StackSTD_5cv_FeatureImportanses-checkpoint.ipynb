{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTANT !\n",
    "\n",
    "# In the first order need to set the number of CPU \n",
    "# for calculation before launching (depends on computer's number of cores)\n",
    "n_jobs= 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import joblib\n",
    "import warnings\n",
    "from datetime import date, datetime\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "import scipy.stats as st\n",
    "\n",
    "from nilearn import image as nli\n",
    "from nilearn import plotting\n",
    "\n",
    "from mne.viz import plot_connectivity_circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_features(table_in, control, index): \n",
    "    #table_in should be a table of features, where rows - subjects, columns - features\n",
    "    \n",
    "    if len(table_in.values.shape) == 1: #for pd.Series # for target\n",
    "        \n",
    "        #shrink data to local train index\n",
    "        table_in = table_in.reindex(index = index)\n",
    "        control = control.reindex(index = index)\n",
    "        ind = table_in.index\n",
    "        \n",
    "        #loop\n",
    "        dct_table = {}\n",
    "        dct_lin_models ={}\n",
    "        dct_std_y_models ={}\n",
    "        \n",
    "        col='0'\n",
    "        \n",
    "        y = table_in #target, brain ROI\n",
    "        X = control  #features, like age, sex and/or movements\n",
    "\n",
    "        #Standartize target\n",
    "        std_model_y = StandardScaler()\n",
    "        std_model_y.fit(y.values.reshape(-1, 1))\n",
    "        y = std_model_y.transform(y.values.reshape(-1, 1))\n",
    "        \n",
    "        #reshaping data\n",
    "        if len(X.values.shape) == 1:\n",
    "            X = X.values.reshape(-1, 1)\n",
    "        else:\n",
    "            X = X.values\n",
    "        y = y.reshape(-1, 1).ravel()\n",
    "        \n",
    "        #Standartize X\n",
    "        std_model = StandardScaler()\n",
    "        std_model.fit(X)\n",
    "        X = std_model.transform(X)\n",
    "\n",
    "        #Fit to the training set\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        y_pred = model.predict(X)\n",
    "\n",
    "        y_res = y - y_pred\n",
    "\n",
    "        dct_table[col] = y_res\n",
    "        dct_lin_models[col] = model\n",
    "        dct_std_y_models[col] = std_model_y\n",
    "\n",
    "        df_table = pd.DataFrame(dct_table, index = ind)\n",
    "\n",
    "        \n",
    "    else:\n",
    "            \n",
    "        #shrink data to local train index\n",
    "        table_in = table_in.reindex(index = index)\n",
    "        control = control.reindex(index = index)\n",
    "        ind = table_in.index\n",
    "\n",
    "        #loop\n",
    "        dct_table = {}\n",
    "        dct_lin_models ={}\n",
    "        dct_std_y_models ={}\n",
    "        col_names = table_in.columns\n",
    "\n",
    "        for col in col_names:\n",
    "            y = table_in[col] #target, brain ROI\n",
    "            X = control  #features, like age, sex and/or movements\n",
    "            \n",
    "            #Standartize target\n",
    "            std_model_y = StandardScaler()\n",
    "            std_model_y.fit(y.values.reshape(-1, 1))\n",
    "            y = std_model_y.transform(y.values.reshape(-1, 1)) \n",
    "            \n",
    "            #reshaping data\n",
    "            if len(X.values.shape) == 1:\n",
    "                X = X.values.reshape(-1, 1)\n",
    "            else:\n",
    "                X = X.values\n",
    "            y = y.reshape(-1, 1).ravel()\n",
    "            \n",
    "            #Standartize X\n",
    "            std_model = StandardScaler()\n",
    "            std_model.fit(X)\n",
    "            X = std_model.transform(X)\n",
    "\n",
    "            #Fit to the training set\n",
    "            model = LinearRegression()\n",
    "            model.fit(X, y)\n",
    "            y_pred = model.predict(X)\n",
    "\n",
    "            y_res = y - y_pred\n",
    "\n",
    "            dct_table[col] = y_res\n",
    "            dct_lin_models[col] = model\n",
    "            dct_std_y_models[col] = std_model_y\n",
    "\n",
    "        df_table = pd.DataFrame(dct_table, index = ind)\n",
    "    \n",
    "    return df_table, dct_std_y_models, std_model, dct_lin_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_control_features(table_in, control, index, dct_std_y_models, std_model, dct_lin_models):\n",
    "    \n",
    "    if len(table_in.values.shape) == 1: #for pd.Series # for target\n",
    "        \n",
    "        #shrink data to local train index\n",
    "        table_in = table_in.reindex(index = index)\n",
    "        control = control.reindex(index = index)\n",
    "        ind = table_in.index\n",
    "        \n",
    "        #loop\n",
    "        dct_table = {}\n",
    "        \n",
    "        col='0'\n",
    "        \n",
    "        y = table_in #target, brain ROI\n",
    "        X = control  #features, like age, sex and/or movements\n",
    "        \n",
    "        #standartize y\n",
    "        y = dct_std_y_models[col].transform(y.values.reshape(-1, 1))\n",
    "        \n",
    "        #reshaping data\n",
    "        if len(X.values.shape) == 1:\n",
    "            X = X.values.reshape(-1, 1)\n",
    "        else:\n",
    "            X = X.values\n",
    "        y = y.reshape(-1, 1).ravel()\n",
    "\n",
    "        #Standartize X with previous std model\n",
    "        X = std_model.transform(X)\n",
    "\n",
    "        #Fit with previous LinReg model\n",
    "        y_pred =  dct_lin_models[col].predict(X)\n",
    "\n",
    "        y_res = y - y_pred\n",
    "\n",
    "        dct_table[col] = y_res\n",
    "\n",
    "        df_table = pd.DataFrame(dct_table, index = ind)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #shrink data to local train index\n",
    "        table_in = table_in.reindex(index = index)\n",
    "        control = control.reindex(index = index)\n",
    "        ind = table_in.index\n",
    "\n",
    "        #loop\n",
    "        dct_table = {}\n",
    "        col_names = table_in.columns\n",
    "\n",
    "        for col in col_names:\n",
    "            y = table_in[col] #target, brain ROI\n",
    "            X = control  #features, like age, sex and/or movements\n",
    "\n",
    "            #standartize y\n",
    "            y = dct_std_y_models[col].transform(y.values.reshape(-1, 1))\n",
    "            \n",
    "            #reshaping data\n",
    "            if len(X.values.shape) == 1:\n",
    "                X = X.values.reshape(-1, 1)\n",
    "            else:\n",
    "                X = X.values\n",
    "            y = y.reshape(-1, 1).ravel()\n",
    "\n",
    "            #Standartize X with previous std model\n",
    "            X = std_model.transform(X)\n",
    "\n",
    "            #Fit with previous LinReg model\n",
    "            y_pred =  dct_lin_models[col].predict(X)\n",
    "\n",
    "            y_res = y - y_pred\n",
    "\n",
    "            dct_table[col] = y_res\n",
    "\n",
    "        df_table = pd.DataFrame(dct_table, index = ind)\n",
    "        \n",
    "    return df_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elnet(X, y):\n",
    "\n",
    "    #drop Nan in target and clean this subj from features\n",
    "    y = y.dropna()\n",
    "    X = X.loc[y.index,:]\n",
    "    ind_y = np.array(y.index)\n",
    "      \n",
    "    y_real=y\n",
    "    \n",
    "    #reshaping data\n",
    "    X = X.values\n",
    "    y = y.values.reshape(-1, 1).ravel()\n",
    "    \n",
    "    # Setup the pipeline steps:\n",
    "    steps = [('elasticnet', ElasticNet(random_state=42))]\n",
    "\n",
    "    # Create the pipeline: pipeline \n",
    "    pipeline = Pipeline(steps)\n",
    "\n",
    "    # Specify the hyperparameter space\n",
    "    parameters = {'elasticnet__alpha': np.logspace(-1, 2, 70),\n",
    "                  'elasticnet__l1_ratio':np.linspace(0,1,25)}\n",
    "\n",
    "    # Create the GridSearchCV object:\n",
    "    gm_cv = GridSearchCV(pipeline, parameters, cv=5, n_jobs=n_jobs)\n",
    "    \n",
    "    # Fit to the training set\n",
    "    gm_cv.fit(X, y)\n",
    "    \n",
    "    #predict new y\n",
    "    y_pred = gm_cv.predict(X)\n",
    "\n",
    "    # Compute and print the metrics\n",
    "    acc = gm_cv.best_score_\n",
    "    bpar = gm_cv.best_params_\n",
    "    model = gm_cv.best_estimator_\n",
    "    mse = mean_squared_error(y_real, y_pred)\n",
    "    mae = mean_absolute_error(y_real, y_pred)\n",
    "    corr, _ = pearsonr(np.array(y_real.values.reshape(-1, 1).ravel(), dtype=float), np.array(y_pred, dtype=float))\n",
    "            \n",
    "    return bpar['elasticnet__alpha'], bpar['elasticnet__l1_ratio'], acc, mse, corr, model, y_pred, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reaply_ElNet(X, y, model):\n",
    "    # param should be pd.Series with indexes from model\n",
    "    \n",
    "    #drop Nan in target and clean this subj from features\n",
    "    y = y.dropna()\n",
    "    X = X.reindex(index =y.index)\n",
    "    ind_y = np.array(y.index)  # indexes as separate variable \n",
    "    \n",
    "    y_real = y\n",
    "\n",
    "    #reshaping data\n",
    "    X = X.values\n",
    "    y = y.values.reshape(-1, 1).ravel()\n",
    "    \n",
    "    #predict new y\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Compute and print the metrics\n",
    "    bacc = model.score(X, y)\n",
    "    mse = mean_squared_error(y_real, y_pred)\n",
    "    mae = mean_absolute_error(y_real, y_pred) \n",
    "    corr, _ = pearsonr(np.array(y_real.values.reshape(-1, 1).ravel(), dtype=float), np.array(y_pred, dtype=float))\n",
    "    \n",
    "    return y_pred, y_real, ind_y, bacc, mse, corr, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to the tables folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/media/data/HCPAging/data/MLTablesMultCon/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "#demography\n",
    "demo = pd.read_csv(path+'demography.csv', index_col=0)\n",
    "\n",
    "#targets table\n",
    "targ = pd.read_csv(path+'cognition.csv', index_col=0)\n",
    "\n",
    "#features tables as dictionary\n",
    "features = {\n",
    "    'carit1':pd.read_csv(path+'carit-con1.csv', index_col=0),\n",
    "    'carit3':pd.read_csv(path+'carit-con3.csv', index_col=0),\n",
    "    'carit4':pd.read_csv(path+'carit-con4.csv', index_col=0),\n",
    "    \n",
    "    'face1':pd.read_csv(path+'FACENAME_group_table_3EV_con1.csv', index_col=0),\n",
    "    'face2':pd.read_csv(path+'FACENAME_group_table_3EV_con2.csv', index_col=0),\n",
    "    'face3':pd.read_csv(path+'FACENAME_group_table_3EV_con3.csv', index_col=0),\n",
    "    'face4':pd.read_csv(path+'FACENAME_group_table_3EV_con4.csv', index_col=0),\n",
    "    'face5':pd.read_csv(path+'FACENAME_group_table_3EV_con5.csv', index_col=0),\n",
    "    'face6':pd.read_csv(path+'FACENAME_group_table_3EV_con6.csv', index_col=0),\n",
    "    \n",
    "    'vism':pd.read_csv(path+'vism.csv', index_col=0),\n",
    "    \n",
    "    'carit_FC':pd.read_csv(path+'CARIT_taskFC.csv', index_col=0),\n",
    "    'face_FC':pd.read_csv(path+'FACENAME_task_FC_3EV.csv', index_col=0),\n",
    "    'vism_FC':pd.read_csv(path+'VISMOTOR_taskFC.csv', index_col=0),\n",
    "\n",
    "    'cort':pd.read_csv(path+'cort.csv', index_col=0),\n",
    "    'surf':pd.read_csv(path+'surf.csv', index_col=0),\n",
    "    'subc':pd.read_csv(path+'subc.csv', index_col=0),\n",
    "    'VolBrain':pd.read_csv(path+'VolBrain.csv', index_col=0),\n",
    "    \n",
    "    'rest':pd.read_csv(path+'rest_hpass.csv', index_col=0) \n",
    "\n",
    "}\n",
    "\n",
    "#table with movements (mean relative displacement Movement_RelativeRMS_mean.txt)\n",
    "movements = pd.read_csv(path+'movements.csv', index_col=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tables withcontroling parameters\n",
    "sex_coded = pd.Series(LabelEncoder().fit_transform(demo.loc[:,['sex']]), index=demo.index, name='sex')\n",
    "\n",
    "control = pd.DataFrame({'sex':sex_coded}) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nih_totalcogcomp_unadjusted</th>\n",
       "      <th>nih_crycogcomp_unadjusted</th>\n",
       "      <th>nih_fluidcogcomp_unadjusted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HCA6002236</th>\n",
       "      <td>112</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCA6018857</th>\n",
       "      <td>109</td>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCA6030645</th>\n",
       "      <td>96</td>\n",
       "      <td>103</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCA6047359</th>\n",
       "      <td>120</td>\n",
       "      <td>123</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCA6051047</th>\n",
       "      <td>101</td>\n",
       "      <td>108</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCA9943504</th>\n",
       "      <td>93</td>\n",
       "      <td>101</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCA9947411</th>\n",
       "      <td>93</td>\n",
       "      <td>92</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCA9953406</th>\n",
       "      <td>116</td>\n",
       "      <td>118</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCA9956008</th>\n",
       "      <td>121</td>\n",
       "      <td>118</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCA9992517</th>\n",
       "      <td>102</td>\n",
       "      <td>112</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>504 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            nih_totalcogcomp_unadjusted  nih_crycogcomp_unadjusted  \\\n",
       "subject                                                              \n",
       "HCA6002236                          112                        111   \n",
       "HCA6018857                          109                        108   \n",
       "HCA6030645                           96                        103   \n",
       "HCA6047359                          120                        123   \n",
       "HCA6051047                          101                        108   \n",
       "...                                 ...                        ...   \n",
       "HCA9943504                           93                        101   \n",
       "HCA9947411                           93                         92   \n",
       "HCA9953406                          116                        118   \n",
       "HCA9956008                          121                        118   \n",
       "HCA9992517                          102                        112   \n",
       "\n",
       "            nih_fluidcogcomp_unadjusted  \n",
       "subject                                  \n",
       "HCA6002236                          111  \n",
       "HCA6018857                          108  \n",
       "HCA6030645                           91  \n",
       "HCA6047359                          111  \n",
       "HCA6051047                           95  \n",
       "...                                 ...  \n",
       "HCA9943504                           88  \n",
       "HCA9947411                           97  \n",
       "HCA9953406                          110  \n",
       "HCA9956008                          119  \n",
       "HCA9992517                           92  \n",
       "\n",
       "[504 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interview_age</th>\n",
       "      <th>sex</th>\n",
       "      <th>race</th>\n",
       "      <th>ethnic_group</th>\n",
       "      <th>family_user_def_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HCA6002236</th>\n",
       "      <td>558</td>\n",
       "      <td>F</td>\n",
       "      <td>White</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>1973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCA6018857</th>\n",
       "      <td>436</td>\n",
       "      <td>F</td>\n",
       "      <td>Unknown or not reported</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>1804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCA6030645</th>\n",
       "      <td>544</td>\n",
       "      <td>F</td>\n",
       "      <td>Black or African American</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>1618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCA6047359</th>\n",
       "      <td>640</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>1727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCA6051047</th>\n",
       "      <td>725</td>\n",
       "      <td>F</td>\n",
       "      <td>Black or African American</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>9058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCA9943504</th>\n",
       "      <td>742</td>\n",
       "      <td>F</td>\n",
       "      <td>White</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>1551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCA9947411</th>\n",
       "      <td>459</td>\n",
       "      <td>M</td>\n",
       "      <td>Black or African American</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCA9953406</th>\n",
       "      <td>567</td>\n",
       "      <td>F</td>\n",
       "      <td>White</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>1181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCA9956008</th>\n",
       "      <td>492</td>\n",
       "      <td>F</td>\n",
       "      <td>White</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>1121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCA9992517</th>\n",
       "      <td>648</td>\n",
       "      <td>F</td>\n",
       "      <td>White</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>1432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>504 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            interview_age sex                       race  \\\n",
       "subject                                                    \n",
       "HCA6002236            558   F                      White   \n",
       "HCA6018857            436   F    Unknown or not reported   \n",
       "HCA6030645            544   F  Black or African American   \n",
       "HCA6047359            640   M                      White   \n",
       "HCA6051047            725   F  Black or African American   \n",
       "...                   ...  ..                        ...   \n",
       "HCA9943504            742   F                      White   \n",
       "HCA9947411            459   M  Black or African American   \n",
       "HCA9953406            567   F                      White   \n",
       "HCA9956008            492   F                      White   \n",
       "HCA9992517            648   F                      White   \n",
       "\n",
       "                      ethnic_group  family_user_def_id  \n",
       "subject                                                 \n",
       "HCA6002236  Not Hispanic or Latino                1973  \n",
       "HCA6018857      Hispanic or Latino                1804  \n",
       "HCA6030645  Not Hispanic or Latino                1618  \n",
       "HCA6047359  Not Hispanic or Latino                1727  \n",
       "HCA6051047  Not Hispanic or Latino                9058  \n",
       "...                            ...                 ...  \n",
       "HCA9943504  Not Hispanic or Latino                1551  \n",
       "HCA9947411  Not Hispanic or Latino                1986  \n",
       "HCA9953406  Not Hispanic or Latino                1181  \n",
       "HCA9956008      Hispanic or Latino                1121  \n",
       "HCA9992517  Not Hispanic or Latino                1432  \n",
       "\n",
       "[504 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Leave-P-group out based on n-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nih_fluidcogcomp_unadjusted\n",
      " \n",
      "started to calculate the Fold # 0\n",
      "2023-07-30 15:01:16.722539\n",
      " \n",
      "start 1st level  2023-07-30 15:01:16.753318\n",
      "controlling  carit1 2023-07-30 15:01:16.753767\n",
      "controlling  carit3 2023-07-30 15:01:17.085179\n",
      "controlling  carit4 2023-07-30 15:01:17.363662\n",
      "controlling  face1 2023-07-30 15:01:17.604929\n",
      "controlling  face2 2023-07-30 15:01:17.863268\n",
      "controlling  face3 2023-07-30 15:01:18.113317\n",
      "controlling  face4 2023-07-30 15:01:18.359602\n",
      "controlling  face5 2023-07-30 15:01:18.613892\n",
      "controlling  face6 2023-07-30 15:01:18.860664\n",
      "controlling  vism 2023-07-30 15:01:19.104917\n",
      "controlling  carit_FC 2023-07-30 15:01:19.351853\n",
      "controlling  face_FC 2023-07-30 15:02:06.877934\n",
      "controlling  vism_FC 2023-07-30 15:02:55.281157\n",
      "controlling  cort 2023-07-30 15:03:43.380957\n",
      "controlling  surf 2023-07-30 15:03:43.534909\n",
      "controlling  subc 2023-07-30 15:03:43.672300\n",
      "controlling  VolBrain 2023-07-30 15:03:43.687643\n",
      "controlling  rest 2023-07-30 15:03:43.692264\n",
      "standartize  carit1 2023-07-30 15:06:13.697317\n",
      "standartize  carit3 2023-07-30 15:06:13.704991\n",
      "standartize  carit4 2023-07-30 15:06:13.707747\n",
      "standartize  face1 2023-07-30 15:06:13.709848\n",
      "standartize  face2 2023-07-30 15:06:13.711779\n",
      "standartize  face3 2023-07-30 15:06:13.713702\n",
      "standartize  face4 2023-07-30 15:06:13.715621\n",
      "standartize  face5 2023-07-30 15:06:13.717551\n",
      "standartize  face6 2023-07-30 15:06:13.719495\n",
      "standartize  vism 2023-07-30 15:06:13.721433\n",
      "standartize  carit_FC 2023-07-30 15:06:13.723376\n",
      "standartize  face_FC 2023-07-30 15:06:14.004036\n",
      "standartize  vism_FC 2023-07-30 15:06:14.278567\n",
      "standartize  cort 2023-07-30 15:06:14.554328\n",
      "standartize  surf 2023-07-30 15:06:14.555752\n",
      "standartize  subc 2023-07-30 15:06:14.556765\n",
      "standartize  VolBrain 2023-07-30 15:06:14.557535\n",
      "standartize  rest 2023-07-30 15:06:14.558261\n",
      "reduction  rest 2023-07-30 15:12:01.304611\n",
      "reduction  carit_FC 2023-07-30 15:12:02.312517\n",
      "reduction  face_FC 2023-07-30 15:12:03.266006\n",
      "reduction  vism_FC 2023-07-30 15:12:04.172939\n",
      "standartize PC table  rest 2023-07-30 15:12:05.500905\n",
      "standartize PC table  carit_FC 2023-07-30 15:12:05.545277\n",
      "standartize PC table  face_FC 2023-07-30 15:12:05.584661\n",
      "standartize PC table  vism_FC 2023-07-30 15:12:05.624006\n",
      "start  carit1 2023-07-30 15:12:05.708398\n",
      "start  carit3 2023-07-30 15:12:16.743981\n",
      "start  carit4 2023-07-30 15:12:25.783545\n",
      "start  face1 2023-07-30 15:12:34.905998\n",
      "start  face2 2023-07-30 15:12:43.978517\n",
      "start  face3 2023-07-30 15:12:52.497429\n",
      "start  face4 2023-07-30 15:13:02.082833\n",
      "start  face5 2023-07-30 15:13:11.773195\n",
      "start  face6 2023-07-30 15:13:19.831186\n",
      "start  vism 2023-07-30 15:13:29.056279\n",
      "start  carit_FC 2023-07-30 15:13:38.520243\n",
      "start  face_FC 2023-07-30 15:13:44.558578\n",
      "start  vism_FC 2023-07-30 15:13:50.476750\n",
      "start  cort 2023-07-30 15:13:58.000468\n",
      "start  surf 2023-07-30 15:14:06.570130\n",
      "start  subc 2023-07-30 15:14:12.816320\n",
      "start  VolBrain 2023-07-30 15:14:18.905598\n",
      "start  rest 2023-07-30 15:14:26.537885\n",
      " \n",
      "start 2nd level  2023-07-30 15:14:34.169768\n",
      "Checking single ML on train 40prc data  2023-07-30 15:14:34.169802\n",
      "controlling  carit1 2023-07-30 15:14:34.170338\n",
      "controlling  carit3 2023-07-30 15:14:34.239936\n",
      "controlling  carit4 2023-07-30 15:14:34.303267\n",
      "controlling  face1 2023-07-30 15:14:34.366635\n",
      "controlling  face2 2023-07-30 15:14:34.430928\n",
      "controlling  face3 2023-07-30 15:14:34.494844\n",
      "controlling  face4 2023-07-30 15:14:34.558403\n",
      "controlling  face5 2023-07-30 15:14:34.621915\n",
      "controlling  face6 2023-07-30 15:14:34.686691\n",
      "controlling  vism 2023-07-30 15:14:34.750530\n",
      "controlling  carit_FC 2023-07-30 15:14:34.813767\n",
      "controlling  face_FC 2023-07-30 15:14:48.988932\n",
      "controlling  vism_FC 2023-07-30 15:15:02.200413\n",
      "controlling  cort 2023-07-30 15:15:15.644604\n",
      "controlling  surf 2023-07-30 15:15:15.691209\n",
      "controlling  subc 2023-07-30 15:15:15.716344\n",
      "controlling  VolBrain 2023-07-30 15:15:15.720361\n",
      "controlling  rest 2023-07-30 15:15:15.722186\n",
      "standartize  carit1 2023-07-30 15:15:29.180358\n",
      "standartize  carit3 2023-07-30 15:15:29.186864\n",
      "standartize  carit4 2023-07-30 15:15:29.192982\n",
      "standartize  face1 2023-07-30 15:15:29.199375\n",
      "standartize  face2 2023-07-30 15:15:29.205610\n",
      "standartize  face3 2023-07-30 15:15:29.207458\n",
      "standartize  face4 2023-07-30 15:15:29.208443\n",
      "standartize  face5 2023-07-30 15:15:29.209364\n",
      "standartize  face6 2023-07-30 15:15:29.210336\n",
      "standartize  vism 2023-07-30 15:15:29.211117\n",
      "standartize  carit_FC 2023-07-30 15:15:29.212054\n",
      "standartize  face_FC 2023-07-30 15:15:29.332050\n",
      "standartize  vism_FC 2023-07-30 15:15:29.456914\n",
      "standartize  cort 2023-07-30 15:15:29.581503\n",
      "standartize  surf 2023-07-30 15:15:29.582466\n",
      "standartize  subc 2023-07-30 15:15:29.583166\n",
      "standartize  VolBrain 2023-07-30 15:15:29.583748\n",
      "standartize  rest 2023-07-30 15:15:29.584274\n",
      "reduction  rest 2023-07-30 15:21:34.160859\n",
      "reduction  carit_FC 2023-07-30 15:21:34.296399\n",
      "reduction  face_FC 2023-07-30 15:21:34.435471\n",
      "reduction  vism_FC 2023-07-30 15:21:34.559789\n",
      "standartize PCA  rest 2023-07-30 15:21:34.686962\n",
      "standartize PCA  carit_FC 2023-07-30 15:21:34.734136\n",
      "standartize PCA  face_FC 2023-07-30 15:21:34.778092\n",
      "standartize PCA  vism_FC 2023-07-30 15:21:34.815329\n",
      "Calculating stacked ML on train 40prc data  2023-07-30 15:21:34.886232\n",
      "set 1 2023-07-30 15:21:34.886353\n",
      "set 2 2023-07-30 15:21:44.012567\n",
      "set 3 2023-07-30 15:21:51.370328\n",
      "set 4 2023-07-30 15:21:58.666628\n",
      "set 5 2023-07-30 15:22:06.865604\n",
      "set 6 2023-07-30 15:22:13.533509\n",
      "set 7 2023-07-30 15:22:20.613015\n",
      "set 8 2023-07-30 15:22:27.499739\n",
      " \n",
      "start 3rd level  2023-07-30 15:22:34.397894\n",
      "Checking single ML on test data  2023-07-30 15:22:34.397935\n",
      "controlling  carit1 2023-07-30 15:22:34.398306\n",
      "controlling  carit3 2023-07-30 15:22:34.471029\n",
      "controlling  carit4 2023-07-30 15:22:34.533479\n",
      "controlling  face1 2023-07-30 15:22:34.596512\n",
      "controlling  face2 2023-07-30 15:22:34.660492\n",
      "controlling  face3 2023-07-30 15:22:34.724013\n",
      "controlling  face4 2023-07-30 15:22:34.787316\n",
      "controlling  face5 2023-07-30 15:22:34.850958\n",
      "controlling  face6 2023-07-30 15:22:34.916040\n",
      "controlling  vism 2023-07-30 15:22:34.979724\n",
      "controlling  carit_FC 2023-07-30 15:22:35.042568\n",
      "controlling  face_FC 2023-07-30 15:22:48.682026\n",
      "controlling  vism_FC 2023-07-30 15:23:02.801409\n",
      "controlling  cort 2023-07-30 15:23:16.160342\n",
      "controlling  surf 2023-07-30 15:23:16.207212\n",
      "controlling  subc 2023-07-30 15:23:16.233141\n",
      "controlling  VolBrain 2023-07-30 15:23:16.237070\n",
      "controlling  rest 2023-07-30 15:23:16.238529\n",
      "standartize  carit1 2023-07-30 15:23:30.347574\n",
      "standartize  carit3 2023-07-30 15:23:30.353742\n",
      "standartize  carit4 2023-07-30 15:23:30.359604\n",
      "standartize  face1 2023-07-30 15:23:30.365804\n",
      "standartize  face2 2023-07-30 15:23:30.371940\n",
      "standartize  face3 2023-07-30 15:23:30.373587\n",
      "standartize  face4 2023-07-30 15:23:30.374367\n",
      "standartize  face5 2023-07-30 15:23:30.375141\n",
      "standartize  face6 2023-07-30 15:23:30.375922\n",
      "standartize  vism 2023-07-30 15:23:30.376690\n",
      "standartize  carit_FC 2023-07-30 15:23:30.377474\n",
      "standartize  face_FC 2023-07-30 15:23:30.497758\n",
      "standartize  vism_FC 2023-07-30 15:23:30.613494\n",
      "standartize  cort 2023-07-30 15:23:30.726642\n",
      "standartize  surf 2023-07-30 15:23:30.727428\n",
      "standartize  subc 2023-07-30 15:23:30.727932\n",
      "standartize  VolBrain 2023-07-30 15:23:30.728312\n",
      "standartize  rest 2023-07-30 15:23:30.728675\n",
      "reduction  rest 2023-07-30 15:29:29.892677\n",
      "reduction  carit_FC 2023-07-30 15:29:30.038649\n",
      "reduction  face_FC 2023-07-30 15:29:30.185208\n",
      "reduction  vism_FC 2023-07-30 15:29:30.305330\n",
      "standartize PCA  rest 2023-07-30 15:29:30.421261\n",
      "standartize PCA  carit_FC 2023-07-30 15:29:30.466285\n",
      "standartize PCA  face_FC 2023-07-30 15:29:30.508005\n",
      "standartize PCA  vism_FC 2023-07-30 15:29:30.549971\n",
      "Calculating stacked ML on test2 data  2023-07-30 15:29:30.620794\n",
      " \n",
      "finished to calculate the Fold # 0\n",
      "2023-07-30 15:29:30.854081\n",
      "nih_fluidcogcomp_unadjusted\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/media/data/HCPAging/data/MLTablesMultCon/output_5cv_sexAdj_noStdTarg_STDstackFeatures_SHAP_nih_fluidcogcomp_unadjusted'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_63255/1446768733.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m###make folder for outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mnmf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'output_5cv_sexAdj_noStdTarg_STDstackFeatures_SHAP_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnmf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/media/data/HCPAging/data/MLTablesMultCon/output_5cv_sexAdj_noStdTarg_STDstackFeatures_SHAP_nih_fluidcogcomp_unadjusted'"
     ]
    }
   ],
   "source": [
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#for COL in targ.columns:\n",
    "COL = 'nih_fluidcogcomp_unadjusted'  #the script adapted to be launched on table of target variables. To launch in that way you need to uncomment for loop and comment this row with col variable\n",
    "y = targ[COL]\n",
    "\n",
    "print(y.name)\n",
    "\n",
    "###make folder for outputs\n",
    "nmf=path+'output_5cv_sexAdj_noStdTarg_STDstackFeatures_SHAP_'+y.name\n",
    "os.mkdir(nmf)\n",
    "\n",
    "i=0\n",
    "\n",
    "#group_kfold = GroupKFold(n_splits=5) #number of folds\n",
    "#for train_index, test_index in group_kfold.split(demo, groups=demo['family_user_def_id']): #based on families id \n",
    "\n",
    "print(' ')\n",
    "print('started to calculate the Fold #', i)\n",
    "print(datetime.now())\n",
    "print(' ')\n",
    "\n",
    "###create directory for specific Fold\n",
    "os.mkdir(nmf+'/Fold_'+str(i)) \n",
    "path_out = str(nmf+'/Fold_'+str(i))\n",
    "\n",
    "###Global indices\n",
    "#train_index = np.array(demo.iloc[train_index].index) #for training all models\n",
    "#test_index = np.array(demo.iloc[test_index].index) #for final test\n",
    "train_index = np.array(demo.index) #for training all models\n",
    "test_index = np.array(demo.index) #for final test\n",
    "\n",
    "###Split global to local indices (in case if we need to split training part into two as previously)\n",
    "#index_train, index_test = train_test_split(train_index, test_size=0.4, random_state=42)\n",
    "#index_train = np.array(sorted(index_train)) #for training modalities models\n",
    "#index_test = np.array(sorted(index_test)) #for testing modalities and training stacking\n",
    "\n",
    "### 1st level ################################################################################\n",
    "\n",
    "#### Calculations of single ML models on training index #################################### \n",
    "\n",
    "print('start 1st level ', datetime.now())\n",
    "\n",
    "\n",
    "#reindex y (target)\n",
    "y_res1 = y.reindex(index=train_index)\n",
    "\n",
    "\n",
    "#control modalities\n",
    "features_res1 = {}\n",
    "std_feat_y_dct = {}\n",
    "std_feat_X_dct = {}\n",
    "linreg_feat_dct = {}\n",
    "for key in features.keys():\n",
    "    print('controlling ', key, datetime.now())\n",
    "\n",
    "    mod_res, std_f_y, std_f_X, linreg_f = control_features(features[key], control, y_res1.index)\n",
    "\n",
    "    features_res1[key] = mod_res\n",
    "    std_feat_y_dct[key] = std_f_y\n",
    "    std_feat_X_dct[key] = std_f_X\n",
    "    linreg_feat_dct[key] = linreg_f\n",
    "\n",
    "#save adjastment model\n",
    "os.mkdir(path_out+'/adjustment_models')\n",
    "#features model\n",
    "joblib.dump(std_feat_y_dct, (path_out+'/adjustment_models'+'/features_std_model_y.sav'))\n",
    "joblib.dump(std_feat_X_dct, (path_out+'/adjustment_models'+'/features_std_model_X.sav'))\n",
    "joblib.dump(linreg_feat_dct, (path_out+'/adjustment_models'+'/features_linreg.sav'))\n",
    "\n",
    "\n",
    "###standartize before model and keep std models\n",
    "#features\n",
    "std_models_features = {}\n",
    "for key in features_res1.keys():\n",
    "    print('standartize ', key, datetime.now())\n",
    "    std_model = StandardScaler()\n",
    "    std_model.fit(features_res1[key].values)\n",
    "    features_res1[key] = pd.DataFrame(std_model.transform(features_res1[key].values),\n",
    "                                      index=features_res1[key].index, \n",
    "                                      columns=features_res1[key].columns)\n",
    "    std_models_features[key] = std_model\n",
    "\n",
    "\n",
    "#save \n",
    "os.mkdir(path_out+'/standartization_models')\n",
    "#features\n",
    "joblib.dump(std_models_features,  (path_out+'/standartization_models'+'/features_std_model.sav'))\n",
    "\n",
    "\n",
    "#save features table before PCA\n",
    "y_res1.to_csv(path_out+'/target_y_train1.csv')\n",
    "for key in features_res1.keys():\n",
    "    features_res1[key].to_csv(path_out+'/'+str(key)+'_train1.csv')\n",
    "\n",
    "\n",
    "#PCA models to rest and task FC\n",
    "PCA_models = {}\n",
    "for key in ['rest', 'carit_FC', 'face_FC', 'vism_FC']:\n",
    "    print('reduction ', key, datetime.now())\n",
    "    model_PCA =  PCA(n_components=75, random_state=11)\n",
    "    model_PCA.fit(features_res1[key].values)\n",
    "    features_res1[key] = pd.DataFrame(model_PCA.transform(features_res1[key].values), \n",
    "                                      index=features_res1[key].index)\n",
    "    PCA_models[key] = model_PCA\n",
    "#save PCA models\n",
    "os.mkdir(path_out+'/PCA_models')\n",
    "joblib.dump(PCA_models,  (path_out+'/PCA_models'+'/PCA_model.sav'))\n",
    "\n",
    "\n",
    "#apply new std to PCA features again\n",
    "std_PC_feature_models = {}\n",
    "for key in ['rest', 'carit_FC', 'face_FC', 'vism_FC']:\n",
    "    print('standartize PC table ', key, datetime.now())\n",
    "    std_PC_model = StandardScaler()\n",
    "    std_PC_model.fit(features_res1[key].values)\n",
    "    features_res1[key] = pd.DataFrame(std_PC_model.transform(features_res1[key].values),\n",
    "                                      index=features_res1[key].index, \n",
    "                                      columns=features_res1[key].columns)\n",
    "    std_PC_feature_models[key] = std_PC_model\n",
    "    #save PCA tables\n",
    "    features_res1[key].to_csv(path_out+'/'+key+'_PCA75_train1.csv')\n",
    "#save std PCA models\n",
    "os.mkdir(path_out+'/PCA_standardization_models')\n",
    "joblib.dump(std_PC_feature_models,  (path_out+'/PCA_standardization_models'+'/std_PCA_model.sav'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Launch ElasticNet for all task(modalities) on index_train (1st level)\n",
    "\n",
    "dict_tasks={}\n",
    "dict_elnet_model={}\n",
    "dict_ypred1={}\n",
    "\n",
    "for key in list(features_res1.keys()):\n",
    "\n",
    "    print('start ', str(key), datetime.now())   #print start time of calculations\n",
    "\n",
    "    bpar1, bpar2, acc, mse, corr, model, y_pred1, mae = elnet(features_res1[key], y_res1) #ML\n",
    "    dict_tasks[key] = acc, mse, mae, corr, bpar1, bpar2 \n",
    "    dict_elnet_model[key] = model\n",
    "    dict_ypred1[key] = y_pred1\n",
    "df_tasks = pd.DataFrame(dict_tasks, index=['best score r2', 'mse', 'mae','corr', 'best alpha', 'best l1_ratio'])\n",
    "df_y_pred1 = pd.DataFrame(dict_ypred1, index=y_res1.index)\n",
    "\n",
    "\n",
    "###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "#models\n",
    "for key in dict_elnet_model.keys():\n",
    "    joblib.dump(dict_elnet_model[key], (path_out+'/'+str(key)+'_elnet_model.sav'))\n",
    "\n",
    "#model performance\n",
    "df_tasks.to_csv(path_out+'/1level_train_perf_elnet.csv')\n",
    "\n",
    "#list of first level targets (observed and predicted)\n",
    "df_y_pred1.to_csv(path_out+'/1level_train_y_pred_singleML.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2st level ################################################################################\n",
    "print(' ')\n",
    "print('start 2nd level ', datetime.now())\n",
    "\n",
    "#### L2 Testing single ML models on 40pr of training (in no 60/40 splitting it just repeats previous)###################\n",
    "\n",
    "print('Checking single ML on train 40prc data ', datetime.now())\n",
    "\n",
    "#controlling  with sorting to index_test\n",
    "\n",
    "#reindex y (target) \n",
    "y_res2 = y.reindex(index=train_index)\n",
    "\n",
    "#control modalities\n",
    "features_res2 = {}\n",
    "for key in features.keys():\n",
    "    print('controlling ', key, datetime.now())\n",
    "\n",
    "    features_res2[key] = re_control_features(features[key], control, y_res2.index, \n",
    "                                             std_feat_y_dct[key], std_feat_X_dct[key], linreg_feat_dct[key])\n",
    "\n",
    "###standartize before model and keep std models\n",
    "#features\n",
    "for key in features_res2.keys():\n",
    "    print('standartize ', key, datetime.now())\n",
    "    features_res2[key] = pd.DataFrame(std_models_features[key].transform(features_res2[key].values),\n",
    "                                      index=features_res2[key].index, \n",
    "                                      columns=features_res2[key].columns) \n",
    "\n",
    "#save features table before PCA\n",
    "y_res2.to_csv(path_out+'/target_y_train2.csv')\n",
    "for key in features_res2.keys():\n",
    "    features_res2[key].to_csv(path_out+'/'+str(key)+'_train2.csv')            \n",
    "\n",
    "\n",
    "#PCA models to rest and task FC\n",
    "for key in ['rest', 'carit_FC', 'face_FC', 'vism_FC']:\n",
    "    print('reduction ', key, datetime.now())\n",
    "    features_res2[key] = pd.DataFrame(PCA_models[key].transform(features_res2[key].values),\n",
    "                                      index=features_res2[key].index)\n",
    "\n",
    "\n",
    "#apply new std to PCA features again\n",
    "for key in ['rest', 'carit_FC', 'face_FC', 'vism_FC']:\n",
    "    print('standartize PCA ', key, datetime.now())\n",
    "    features_res2[key] = pd.DataFrame(std_PC_feature_models[key].transform(features_res2[key].values),\n",
    "                                      index=features_res2[key].index, \n",
    "                                      columns=features_res2[key].columns)\n",
    "    #save std pc table\n",
    "    features_res2[key].to_csv(path_out+'/'+key+'_PCA75_train2.csv')\n",
    "\n",
    "\n",
    "#apply trained single models ElasticNet to new subset\n",
    "\n",
    "dict_y_pred2={}\n",
    "dict_y_pred2_per={}\n",
    "for key in list(features_res2.keys()):\n",
    "    y_pred, y_real, ind_y, bacc, mse, corr, mae = reaply_ElNet(features_res2[key], y_res2, dict_elnet_model[key]) #ML\n",
    "    dict_y_pred2[key] = y_pred\n",
    "    dict_y_pred2_per[key] = bacc, mse, mae, corr\n",
    "\n",
    "df_y_pred2 = pd.DataFrame(dict_y_pred2, index=ind_y)\n",
    "df_y_pred2_per = pd.DataFrame(dict_y_pred2_per, index=['best score r2', 'mse', 'mae','corr'])\n",
    "\n",
    "\n",
    "###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "#model performance\n",
    "df_y_pred2_per.to_csv(path_out+'/2level_test1_perf_elnet.csv')\n",
    "\n",
    "#list of first level targets (observed and predicted)\n",
    "df_y_pred2.to_csv(path_out+'/2level_test1_y_pred_singleML.csv')   \n",
    "\n",
    "\n",
    "\n",
    "#### L2 Calculating stacked ML models on 40prc #############################################\n",
    "\n",
    "print('Calculating stacked ML on train 40prc data ', datetime.now())    \n",
    "\n",
    "\n",
    "#identifying sets for several stacked models\n",
    "set2 = ['carit1', 'carit3', 'carit4', 'face1', 'face2', 'face3', 'face4', 'face5', 'face6', 'vism']\n",
    "set3 = ['cort', 'subc', 'surf', 'rest', 'VolBrain']\n",
    "\n",
    "set4 = ['carit_FC', 'face_FC', 'vism_FC']\n",
    "set5 = ['carit1', 'carit3', 'carit4', 'face1', 'face2', 'face3', 'face4', 'face5', 'face6', 'vism', 'carit_FC', 'face_FC', 'vism_FC']\n",
    "set6 = ['carit1', 'carit3', 'carit4', 'face1', 'face2', 'face3', 'face4', 'face5', 'face6', 'vism', 'cort', 'subc', 'surf', 'rest', 'VolBrain']\n",
    "set7 = ['carit_FC', 'face_FC', 'vism_FC', 'cort', 'subc', 'surf', 'rest', 'VolBrain']\n",
    "set8 = ['carit_FC', 'face_FC', 'vism_FC', 'rest']\n",
    "\n",
    "set1 = list(df_y_pred2.columns) #all existed modalities\n",
    "\n",
    "#for presetet sets\n",
    "dict_st_perf1={}\n",
    "dict_st_models={}\n",
    "dict_st_ypred1={}\n",
    "dct_std_mod_for_stack = {} #\n",
    "dct_std_tab_for_stack = {} #\n",
    "dct_std_tab_before_for_stack = {} #\n",
    "\n",
    "s=1\n",
    "for set_n in [set1, set2, set3, set4, set5, set6, set7, set8]:\n",
    "    print('set '+str(s), datetime.now())\n",
    "\n",
    "    st_features = df_y_pred2.loc[:,set_n]\n",
    "    dct_std_tab_before_for_stack['set'+str(s)] = st_features #\n",
    "\n",
    "    stack_std_model = StandardScaler().fit(st_features.values) \n",
    "    dct_std_mod_for_stack['set'+str(s)] = stack_std_model #\n",
    "\n",
    "    std_st_features = pd.DataFrame(stack_std_model.transform(st_features.values), \n",
    "                                   index=st_features.index, columns=st_features.columns) \n",
    "    dct_std_tab_for_stack['set'+str(s)] = std_st_features #\n",
    "\n",
    "\n",
    "\n",
    "    bpar1, bpar2, acc, mse, corr, model, y_pred3, mae = elnet(std_st_features, y_res2) #ML\n",
    "\n",
    "    dict_st_perf1['set'+str(s)] = acc, mse, mae, corr, bpar1, bpar2 \n",
    "    dict_st_models['set'+str(s)] = model\n",
    "    dict_st_ypred1['set'+str(s)] = y_pred3\n",
    "    s+=1\n",
    "\n",
    "df_st_perf1 = pd.DataFrame(dict_st_perf1, index=['best score r2', 'mse', 'mae','corr', 'best alpha', 'best l1_ratio'])\n",
    "df_st_ypred1 = pd.DataFrame(dict_st_ypred1, index=y_res2.index)        \n",
    "\n",
    "###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "#models\n",
    "for key in dict_st_models.keys():\n",
    "    joblib.dump(dict_st_models[key], (path_out+'/'+str(key)+'_stacked_model.sav'))\n",
    "for key in dct_std_mod_for_stack.keys():\n",
    "    joblib.dump(dct_std_mod_for_stack[key], (path_out+'/'+str(key)+'_stacked_STD_model.sav'))\n",
    "\n",
    "#performance and prediction\n",
    "df_st_perf1.to_csv(path_out+'/2level_test1_perf_stacked.csv')\n",
    "df_st_ypred1.to_csv(path_out+'/2level_test1_y_pred_stacked.csv')\n",
    "for key in dct_std_tab_for_stack.keys():\n",
    "    dct_std_tab_for_stack[key].to_csv(path_out+'/2level_stack_y_feature_tab_STD.csv')\n",
    "    dct_std_tab_before_for_stack[key].to_csv(path_out+'/2level_stack_y_feature_tab_beforeSTD.csv')\n",
    "\n",
    "\n",
    "### 3rd level ################################################################################\n",
    "print(' ')\n",
    "print('start 3rd level ', datetime.now())\n",
    "\n",
    "\n",
    "#### L3 Testing single ML models on test_index #############################################\n",
    "\n",
    "print('Checking single ML on test data ', datetime.now())\n",
    "\n",
    "#controlling with sorting to test index\n",
    "\n",
    "#control y (target)\n",
    "y_res3 = y.reindex(index=test_index)\n",
    "\n",
    "#control modalities\n",
    "features_res3 = {}\n",
    "for key in features.keys():\n",
    "    print('controlling ', key, datetime.now())\n",
    "\n",
    "    features_res3[key] = re_control_features(features[key], control, y_res3.index, \n",
    "                                             std_feat_y_dct[key], std_feat_X_dct[key], linreg_feat_dct[key])\n",
    "\n",
    "###standartize before model and keep std models\n",
    "#features\n",
    "for key in features_res3.keys():\n",
    "    print('standartize ', key, datetime.now())\n",
    "    features_res3[key] = pd.DataFrame(std_models_features[key].transform(features_res3[key].values),\n",
    "                                      index=features_res3[key].index, \n",
    "                                      columns=features_res3[key].columns)\n",
    "\n",
    "\n",
    "#save features table before PCA\n",
    "y_res3.to_csv(path_out+'/target_y_test.csv')\n",
    "for key in features_res3.keys():\n",
    "    features_res3[key].to_csv(path_out+'/'+str(key)+'_test.csv')            \n",
    "\n",
    "\n",
    "#PCA models to rest and task FC\n",
    "for key in ['rest', 'carit_FC', 'face_FC', 'vism_FC']:\n",
    "    print('reduction ', key, datetime.now())\n",
    "    features_res3[key] = pd.DataFrame(PCA_models[key].transform(features_res3[key].values),\n",
    "                                      index=features_res3[key].index)\n",
    "\n",
    "\n",
    "#apply new std to PCA features again\n",
    "for key in ['rest', 'carit_FC', 'face_FC', 'vism_FC']:\n",
    "    print('standartize PCA ', key, datetime.now())\n",
    "    features_res3[key] = pd.DataFrame(std_PC_feature_models[key].transform(features_res3[key].values),\n",
    "                                      index=features_res3[key].index, \n",
    "                                      columns=features_res3[key].columns)\n",
    "    #save std pc table\n",
    "    features_res3[key].to_csv(path_out+'/'+key+'_PCA75_test.csv')\n",
    "\n",
    "\n",
    "\n",
    "#apply trained single models ElasticNet to new subset\n",
    "\n",
    "dict_y_pred3={}\n",
    "dict_y_pred3_per={}\n",
    "for key in list(features_res3.keys()):\n",
    "    y_pred, y_real, ind_y, bacc, mse, corr, mae = reaply_ElNet(features_res3[key], y_res3, dict_elnet_model[key]) #ML\n",
    "    dict_y_pred3[key] = y_pred\n",
    "    dict_y_pred3_per[key] = bacc, mse, mae, corr\n",
    "\n",
    "df_y_pred3 = pd.DataFrame(dict_y_pred3, index=ind_y)\n",
    "df_y_pred3_per = pd.DataFrame(dict_y_pred3_per, index=['best score r2', 'mse', 'mae','corr'])\n",
    "\n",
    "\n",
    "###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "#model performance\n",
    "df_y_pred3_per.to_csv(path_out+'/3level_test2_perf_elnet.csv')\n",
    "\n",
    "#list of first level targets (observed and predicted)\n",
    "df_y_pred3.to_csv(path_out+'/3level_test2_y_pred_singleML.csv')        \n",
    "\n",
    "\n",
    "#### L3 Testing stacked ML models on test_index #############################################\n",
    "\n",
    "print('Calculating stacked ML on test2 data ', datetime.now()) \n",
    "\n",
    "#apply trained stacked models ElasticNet to new data , test_index\n",
    "\n",
    "#for presetet sets\n",
    "dict_st_perf2={}\n",
    "dict_st_ypred2={}\n",
    "\n",
    "dct_std3_tab_for_stack = {} #\n",
    "dct_std3_tab_before_for_stack = {} #\n",
    "\n",
    "s=1\n",
    "for set_n in [set1, set2, set3, set4, set5, set6, set7, set8]:\n",
    "\n",
    "    ftrs = df_y_pred3.loc[:, set_n]\n",
    "    dct_std3_tab_before_for_stack['set'+str(s)] = ftrs\n",
    "\n",
    "    std_ftrs = pd.DataFrame(dct_std_mod_for_stack['set'+str(s)].transform(ftrs.values), \n",
    "                            index=ftrs.index,columns=ftrs.columns)\n",
    "    dct_std3_tab_for_stack['set'+str(s)] = std_ftrs\n",
    "\n",
    "    y_pred, y_real, ind_y, bacc, mse, corr, mae = reaply_ElNet(std_ftrs, y_res3, dict_st_models[('set'+str(s))]) #ML\n",
    "    dict_st_ypred2[('set'+str(s))] = y_pred\n",
    "    dict_st_perf2[('set'+str(s))] = bacc, mse, mae, corr\n",
    "    s+=1\n",
    "\n",
    "df_st_ypred2 = pd.DataFrame(dict_st_ypred2, index=ind_y)\n",
    "df_st_perf2 = pd.DataFrame(dict_st_perf2, index=['best score r2', 'mse', 'mae','corr'])        \n",
    "\n",
    "###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "#performance and prediction\n",
    "df_st_perf2.to_csv(path_out+'/3level_test2_perf_stacked.csv')\n",
    "df_st_ypred2.to_csv(path_out+'/3level_test2_y_pred_stacked.csv') \n",
    "for key in dct_std3_tab_for_stack.keys():\n",
    "    dct_std3_tab_for_stack[key].to_csv(path_out+'/3level_stack_y_feature_tab_STD.csv')\n",
    "    dct_std3_tab_before_for_stack[key].to_csv(path_out+'/3level_stack_y_feature_tab_beforeSTD.csv')\n",
    "\n",
    "print(' ')\n",
    "print('finished to calculate the Fold #', i)\n",
    "print(datetime.now())\n",
    "\n",
    "i+=1\n",
    "\n",
    "print(' ')\n",
    "print('finished the MODEL '+COL)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
